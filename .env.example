# =============================================================================
# Environment Variables Configuration
# =============================================================================
#
# Single .env file for both Docker and local development.
#
# Setup:
#   1. Copy this file to .env:  cp .env.example .env
#   2. Fill in your values (especially OPENAI_API_KEY)
#
# Running with Docker:
#   docker compose --profile development up -d
#
# Running locally (without Docker):
#   cd frontend && npm run dev:local
#   (uses dotenv-cli to load ../.env)
#
# IMPORTANT: Never commit .env files to version control!
#
# =============================================================================

# -----------------------------------------------------------------------------
# Application Settings
# -----------------------------------------------------------------------------

# Node.js environment (development | production | test)
# Default: development
NODE_ENV=development

# Port the application runs on
# Default: 8087
PORT=8087

# Hostname to bind to (use 0.0.0.0 for Docker)
# Default: 0.0.0.0
HOSTNAME=0.0.0.0

# -----------------------------------------------------------------------------
# Logging Configuration
# -----------------------------------------------------------------------------

# Log level controls verbosity of application logs
# Options: debug, info, warn, error
# Default: info (development), warn (production when NODE_ENV=production)
# Set to 'info' in production to see operational logs
LOG_LEVEL=info

# -----------------------------------------------------------------------------
# AI Chatbot Configuration (OpenAI)
# -----------------------------------------------------------------------------

# OpenAI API Key (REQUIRED for chatbot functionality)
# Get your key at: https://platform.openai.com/api-keys
# Format: sk-...
OPENAI_API_KEY=

# OpenAI Model to use for chat completions
# Options: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
# Default: gpt-4o-mini (cost-effective, good quality)
# OPENAI_MODEL=gpt-4o-mini

# -----------------------------------------------------------------------------
# Optional: Advanced LLM Settings (set in code, not typically overridden)
# -----------------------------------------------------------------------------
# These are configured with sensible defaults in the LLM client:
#
# Temperature: 0.7 (controls randomness, 0-2)
# Max Tokens: 1024 (maximum response length)
# Timeout: 30000ms (request timeout)
#
# To customize, modify frontend/src/lib/llm-client.ts

# -----------------------------------------------------------------------------
# Private Knowledge Repository (for Portainer/CI deployments)
# -----------------------------------------------------------------------------

# Provide these to fetch private knowledge repo at build time
# KNOWLEDGE_REPO: URL to your private knowledge repo (both formats work):
#   - github.com/username/private-knowledge.git
#   - https://github.com/username/private-knowledge.git
# GITHUB_TOKEN: Fine-grained personal access token with "Contents: Read" permission
# KNOWLEDGE_REPO=github.com/yourusername/private-knowledge.git
# GITHUB_TOKEN=github_pat_xxxxxxxxxxxx

# -----------------------------------------------------------------------------
# Future Configuration (placeholder for upcoming features)
# -----------------------------------------------------------------------------

# Analytics tracking ID (optional)
# ANALYTICS_ID=

# Contact form email recipient (optional)
# CONTACT_EMAIL=
